{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import random\n",
    "from numpy import linalg as LA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdiffeq import odeint_adjoint as odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ising_to_qubo(n, J, h):\n",
    "    # Randomly generate adjacency matrix with 50% chance of each entry being 1\n",
    "    adj_matrix = np.random.randint(0, 2, size=(n, n))  # This line replaces the first nested loop\n",
    "\n",
    "    # Create a diagonal matrix with all diagonal entries equal to h\n",
    "    diagonal_matrix = np.eye(n) * h\n",
    "\n",
    "    # Create the off-diagonal entries based on J and adj_matrix\n",
    "    off_diagonal_matrix = J * adj_matrix\n",
    "    np.fill_diagonal(off_diagonal_matrix, 0)  # Set diagonal entries to 0\n",
    "\n",
    "    # Sum the diagonal and off-diagonal matrices to get the final qubo_matrix\n",
    "    qubo_matrix = diagonal_matrix + off_diagonal_matrix\n",
    "\n",
    "    return qubo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdyn.models import NeuralODE\n",
    "\n",
    "# Define the continuous dynamics for the Neural ODE\n",
    "class ContinuousDynamics(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ContinuousDynamics, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, input_size)\n",
    "\n",
    "    def forward(self, t, x, args=None):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "class RecursiveNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, iterations):\n",
    "        super(RecursiveNN, self).__init__()\n",
    "        self.iterations = iterations\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        self.normalization = nn.LayerNorm(output_size)\n",
    "        self.skip_fc = nn.Linear(input_size, output_size)  # skip connection\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for _ in range(self.iterations):\n",
    "            residual = x  # preserve the input for skip connection\n",
    "            x = F.leaky_relu(self.fc(x))  # using LeakyReLU\n",
    "            x = self.normalization(x) + self.skip_fc(residual)  # added skip connection\n",
    "            outputs.append(x)\n",
    "        return torch.stack(outputs)  # Stack outputs to ensure a consistent tensor shape across batches\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, n, l1_coef=1e-4):  # 1e-4 is the L1 regularization coefficient\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.n = n\n",
    "        self.l1_coef = l1_coef\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        total_loss = 0\n",
    "        weight = 1.0\n",
    "        for output in torch.unbind(outputs, dim=0):  # Unbind along the iteration dimension\n",
    "            reshaped_target = target.view(-1, self.n, self.n)\n",
    "            reshaped_output = torch.sin(output).view(-1, self.n, 1)  # Reshape output to [64, 16, 1]\n",
    "            product = reshaped_target * reshaped_output  # Broadcasting will expand the last dimension of output\n",
    "            total_loss += weight * torch.norm(product)\n",
    "            weight *= 0.9\n",
    "        \n",
    "        l1_reg = sum(p.abs().sum() for p in model.parameters())  # L1 regularization\n",
    "        total_loss += self.l1_coef * l1_reg\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10000\n",
    "i = 64 * 64\n",
    "o = 64\n",
    "\n",
    "x = torch.randn(samples, o) + 1.0j * torch.randn(samples, o)  # Pre-compute random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute random values\n",
    "random_values1 = ((-1)**(np.random.randint(0, 100, size=samples) % 2) * np.random.random(samples))\n",
    "random_values2 = ((-1)**(np.random.randint(0, 100, size=samples) % 2) * np.random.random(samples))\n",
    "\n",
    "# Initialize y\n",
    "y = np.empty((samples, o*o))\n",
    "\n",
    "# Compute input_mat and update y and x in a loop (if vectorization of ising_to_qubo is not possible)\n",
    "for idx in range(samples):\n",
    "    input_mat = ising_to_qubo(o, random_values1[idx], random_values2[idx])\n",
    "    y[idx] = input_mat.flatten()\n",
    "\n",
    "# Normalize and compute angle of x\n",
    "norms = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "x_normalized = x / norms\n",
    "x_angle = np.angle(x_normalized)\n",
    "\n",
    "# Convert to torch tensors\n",
    "y_tensor = torch.from_numpy(y)\n",
    "x_tensor = torch.from_numpy(x_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4912, -0.1094,  0.9030,  ...,  0.0578,  2.5329, -0.0316],\n",
      "        [-2.6746,  1.9726,  1.7977,  ...,  3.1094, -0.1936,  2.6898],\n",
      "        [-0.9938,  2.9547,  3.1292,  ..., -1.6752,  1.5901, -1.6767],\n",
      "        ...,\n",
      "        [-1.7223,  0.4721, -0.8184,  ...,  0.0784, -0.9643, -0.7622],\n",
      "        [-3.0673,  0.1923,  2.1269,  ...,  0.3261,  2.8921,  1.0635],\n",
      "        [ 1.3754, -0.1616,  3.1067,  ..., -0.9389,  2.1736,  1.5637]])\n"
     ]
    }
   ],
   "source": [
    "print(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_size = int(0.8 * samples)\n",
    "val_size = samples - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(TensorDataset(x_tensor, y_tensor), [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "model = RecursiveNN(input_size=o, output_size=o, iterations=50)\n",
    "criterion = CustomLoss(o)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)  # 1e-4 is the L2 regularization coefficient\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1696.9318, Validation Loss: 780.3691\n",
      "Epoch 2/100, Training Loss: 590.6208, Validation Loss: 440.6875\n",
      "Epoch 3/100, Training Loss: 388.4503, Validation Loss: 336.8192\n",
      "Epoch 4/100, Training Loss: 313.5949, Validation Loss: 288.9871\n",
      "Epoch 5/100, Training Loss: 273.7420, Validation Loss: 252.5974\n",
      "Epoch 6/100, Training Loss: 243.9742, Validation Loss: 228.9491\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    scheduler.step()  # Step the learning rate scheduler\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # gradient clipping\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()  # Accumulate training loss\n",
    "\n",
    "    # Calculate the average training loss for this epoch\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # No gradient computation during validation\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()  # Accumulate validation loss\n",
    "    \n",
    "    # Calculate the average validation loss for this epoch\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
